{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visialisation & Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchsampler import ImbalancedDatasetSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data) 91720\n",
      "len(test_data) 7628\n"
     ]
    }
   ],
   "source": [
    "# Transfer data to tensor vectors and training/ testing datasets\n",
    "train_dir = './start_deep/train_images'\n",
    "test_dir = './start_deep/test_images'\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(), \n",
    "     transforms.ToTensor(), \n",
    "     transforms.Normalize(mean=(0,),std=(1,))])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_data = torchvision.datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "valid_size = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "num_train = len(train_data)\n",
    "print(\"len(train_data)\",len(train_data))\n",
    "print (\"len(test_data)\", len(test_data))\n",
    "indices_train = list(range(num_train))\n",
    "np.random.shuffle(indices_train)\n",
    "split_tv = int(np.floor(valid_size * num_train))\n",
    "train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_new_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=1)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "classes = ('noface','face')\n",
    "\n",
    "# for epoch in range(1, n_epochs+1):\n",
    "# for data, target in train_loader:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 26950, 1: 64770}\n"
     ]
    }
   ],
   "source": [
    "print(dict(Counter(train_data.targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 6831, 1: 797}\n"
     ]
    }
   ],
   "source": [
    "print(dict(Counter(test_data.targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_test_face = 0\n",
    "count_test_no_face = 0\n",
    "for data, target in test_loader:\n",
    "    s = np.array(target).sum()\n",
    "    count_test_face +=s\n",
    "    count_test_no_face += batch_size - s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797 6851\n"
     ]
    }
   ],
   "source": [
    "print(count_test_face, count_test_no_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train_face = 0\n",
    "count_train_no_face = 0\n",
    "for data, target in train_loader:\n",
    "    s = np.array(target).sum()\n",
    "    count_train_face +=s\n",
    "    count_train_no_face += batch_size - s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51778 21598\n"
     ]
    }
   ],
   "source": [
    "print(count_train_face, count_train_no_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_valid_face = 0\n",
    "count_valid_no_face = 0\n",
    "for data, target in valid_loader:\n",
    "    s = np.array(target).sum()\n",
    "    count_valid_face +=s\n",
    "    count_valid_no_face += batch_size - s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12992 5376\n"
     ]
    }
   ],
   "source": [
    "print(count_valid_face, count_valid_no_face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Confusion Matrix \n",
    "# Input : output (of model), labels\n",
    "# Output: average error\n",
    "###############################################\n",
    "class ConfusionMatrixMeter(object):     #same as for repSet\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.true_pos = 0\n",
    "        self.true_neg = 0\n",
    "        self.false_pos = 0\n",
    "        self.false_neg = 0\n",
    "\n",
    "    def update(self, output, target):\n",
    "        for i in range(len(output)):\n",
    "            if output[i][0]>output[i][1]: # predicted negative (no face)\n",
    "                if target[i]==0:\n",
    "                    self.true_neg +=1\n",
    "                else:\n",
    "                    self.false_neg +=1\n",
    "            else:                         # predicted positive (face)\n",
    "                if target[i]==1:\n",
    "                    self.true_pos +=1\n",
    "                else:\n",
    "                    self.false_pos +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Description: error function to evaluate distance between prediction and label \n",
    "# Input : output (of model), labels\n",
    "# Output: average error\n",
    "###############################################\n",
    "def error(output, labels):\n",
    "    #print (\"output\",output)\n",
    "    #print (\"labels\",labels)\n",
    "    \n",
    "    correct = 0\n",
    "    predicted = torch.max(output.data, 1)[1] \n",
    "    correct += (predicted == labels).sum()\n",
    "    return 1 - correct.item()/len(output.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Description: class used to keep count of the average error and loss during training\n",
    "# Input : \n",
    "# Output:\n",
    "###############################################\n",
    "class AverageMeter(object):     #same as for repSet\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model 1: default\n",
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(9 * 9 * 64, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, X): # batch of n images\n",
    "        #print (\"input\",X.size())\n",
    "        out = self.layer1(X)\n",
    "        #print (\"after layer1\",out.size())\n",
    "        out = self.layer2(out)\n",
    "        #print (\"after layer2\",out.size())\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #print (\"after reshape\",out.size())\n",
    "        out = self.drop_out(out)\n",
    "        #print (\"after drop\",out.size())\n",
    "        out = self.fc1(out)\n",
    "        #print (\"after fc1\",out.size())\n",
    "        out = self.fc2(out)\n",
    "        #print (\"after fc2\",out.size())\n",
    "        #out = F.log_softmax(out, dim=1)\n",
    "        #print (\"after log softmax\",out.size())\n",
    "        return out "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input torch.Size([32, 1, 36, 36])\n",
    "after layer1 torch.Size([32, 32, 18, 18])\n",
    "after layer2 torch.Size([32, 64, 9, 9])\n",
    "after reshape torch.Size([32, 5184])\n",
    "after drop torch.Size([32, 5184])\n",
    "after fc1 torch.Size([32, 1000])\n",
    "after fc2 torch.Size([32, 2])\n",
    "after log softmax torch.Size([32, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 3\n",
    "lr = 0.001\n",
    "cv_folds = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val iter: 01 epoch: 001 train_loss= 0.27163 train_err= 0.11354 val_loss= 0.09809 val_err= 0.03890 epoch_time= 38.45513\n",
      "Cross-val iter: 01 epoch: 002 train_loss= 0.11648 train_err= 0.04287 val_loss= 0.05773 val_err= 0.02105 epoch_time= 32.87979\n",
      "Cross-val iter: 01 epoch: 003 train_loss= 0.07605 train_err= 0.02356 val_loss= 0.04219 val_err= 0.01658 epoch_time= 33.94685\n",
      "Cross-val iter: 01 test_loss= 0.15697 test_err= 0.04588 cv time= 117.90147\n",
      "517 70\n",
      "280 6761\n"
     ]
    }
   ],
   "source": [
    "for it in range(cv_folds):\n",
    "    \n",
    "    ### Set X and y tensors for training set ###\n",
    "    n_train_batches =200\n",
    "    n_valid_batches = 50\n",
    "    ### Set X and y tensors for testing set ###\n",
    "    n_test_batches = 250\n",
    "        ############################################ - TRAIN, TEST FUNCTIONS\n",
    "    def train(X, y):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss_train = F.cross_entropy(output, y)\n",
    "        #print (output)\n",
    "        #print (y)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        return output, loss_train\n",
    "    \n",
    "    def test(X, y):\n",
    "        output = model(X)\n",
    "        loss_test = F.cross_entropy(output, y)\n",
    "        return output, loss_test \n",
    "\n",
    "    #Initialize Model\n",
    "    model = ConvNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    start = time.perf_counter()       #time indicator \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        startEpoch = time.perf_counter()    #time indicator\n",
    "        train_loss = AverageMeter()\n",
    "        train_err = AverageMeter()\n",
    "        i =0\n",
    "        for data, target in train_loader:\n",
    "            i+=1\n",
    "            if i == n_train_batches:\n",
    "                break\n",
    "            output, loss = train(data, target)\n",
    "            train_loss.update(loss.item(), output.size(0))\n",
    "            train_err.update(error(output, target), output.size(0))\n",
    "            \n",
    "        model.eval()\n",
    "        test_loss = AverageMeter()\n",
    "        test_err = AverageMeter()\n",
    "        i = 0\n",
    "        for data, target in valid_loader:\n",
    "            i+=1\n",
    "            if i == n_valid_batches:\n",
    "                break\n",
    "            output, loss = test(data, target)\n",
    "            test_loss.update(loss.item(), output.size(0))\n",
    "            test_err.update(error(output, target), output.size(0)) \n",
    "        print(\"Cross-val iter:\", '%02d' % (it+1), \"epoch:\", '%03d' % (epoch+1), \"train_loss=\", \"{:.5f}\".format(train_loss.avg),\n",
    "        \"train_err=\", \"{:.5f}\".format(train_err.avg), \"val_loss=\", \"{:.5f}\".format(test_loss.avg),\n",
    "        \"val_err=\", \"{:.5f}\".format(test_err.avg), \"epoch_time=\", \"{:.5f}\".format(time.perf_counter()-startEpoch))\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = AverageMeter()\n",
    "    test_err = AverageMeter()\n",
    "    i=0\n",
    "    cmm = ConfusionMatrixMeter()\n",
    "    for data, target in test_loader:\n",
    "        i+=1\n",
    "        if i == n_test_batches:\n",
    "            break\n",
    "        output, loss = test(data, target)\n",
    "        cmm.update(np.array(output.detach()), np.array(target))\n",
    "        test_loss.update(loss.item(), output.size(0))\n",
    "        test_err.update(error(output.data, target.data), output.size(0)) \n",
    "    print(\"Cross-val iter:\", '%02d' % (it+1),\"test_loss=\", \"{:.5f}\".format(test_loss.avg),\n",
    "        \"test_err=\", \"{:.5f}\".format(test_err.avg), \"cv time=\", \"{:.5f}\".format(time.perf_counter() -start))\n",
    "    print (cmm.true_pos, cmm.false_pos)\n",
    "    print (cmm.false_neg, cmm.true_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
